{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import PIL\n",
    "\n",
    "from pytorch_grad_cam import GradCAM,GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image,preprocess_image\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_img(img):\n",
    "        print(img.dtype)\n",
    "\n",
    "        if np.min(img) < 0 or np.max(img) > 1:\n",
    "                print(\"Values are not in the range [0, 1]\")\n",
    "                min_value = np.min(img)\n",
    "                max_value = np.max(img)\n",
    "\n",
    "                print(\"Minimum pixel value:\", min_value)\n",
    "                print(\"Maximum pixel value:\", max_value)\n",
    "\n",
    "        else:\n",
    "                print(\"Image is valid\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_class_acc(img, model):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),  # Resize to match ResNet's input size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Feed the input tensor through the model\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "\n",
    "    # Apply softmax to get class probabilities\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "    # Get the predicted class label\n",
    "    pred_class = torch.argmax(probs, dim=1).item()\n",
    "    print(\"Predicted class:\", pred_class)\n",
    "    return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_model(model, target_layers, targets, img):\n",
    "    \n",
    "    # instantiate the model\n",
    "    cam = GradCAM(model=model, target_layers=target_layers) # use GradCamPlusPlus class\n",
    "\n",
    "    input_tensor = preprocess_image(img)\n",
    "\n",
    "    # generate CAM\n",
    "    grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "\n",
    "    cam = np.uint8(255*grayscale_cams[0, :])\n",
    "\n",
    "    cam = cv2.merge([cam, cam, cam])\n",
    "\n",
    "    # display the original image & the associated CAM\n",
    "    images = np.hstack((np.uint8(255*img), cam_image))\n",
    "    PIL.Image.fromarray(images)\n",
    "\n",
    "    return cam_image, predicted_class_acc(img, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schizophrenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schizo_model(model, target_layers, targets, img):\n",
    "    \n",
    "    cam = GradCAM(model=model, target_layers=target_layers) # use GradCamPlusPlus class\n",
    "    input_tensor = preprocess_image(img)\n",
    "\n",
    "    # generate CAM\n",
    "    grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "\n",
    "    cam = np.uint8(255*grayscale_cams[0, :])\n",
    "\n",
    "    cam = cv2.merge([cam, cam, cam])\n",
    "\n",
    "    # display the original image & the associated CAM\n",
    "    images = np.hstack((np.uint8(255*img), cam_image))\n",
    "    PIL.Image.fromarray(images)\n",
    "\n",
    "    return cam_image, predicted_class_acc(img, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_blur(model, target_layers, targets, img):\n",
    "\n",
    "    cam = GradCAM(model=model, target_layers=target_layers) # use GradCamPlusPlus class\n",
    "    # Define the location and size of the region to apply the blur\n",
    "    x, y, w, h = (100, 100, 50, 50)  # Example: x and y are the coordinates of the top-left corner, w and h are the width and height\n",
    "\n",
    "    # Apply Gaussian blur to the entire image\n",
    "    blurred_img = cv2.GaussianBlur(img, (25, 25), 0)  # Adjust the kernel size for desired blur intensity\n",
    "\n",
    "    # Create a mask for the square region (all zeros except for the square region which is filled with ones)\n",
    "    mask = np.zeros_like(img)\n",
    "    mask[y:y+h, x:x+w] = 255\n",
    "\n",
    "    # Replace the blurred area with the original square region using the mask\n",
    "    result_img = np.where(mask == 255, img, blurred_img)\n",
    "    img_gauss = result_img\n",
    "\n",
    "    input_tensor = preprocess_image(img_gauss)\n",
    "\n",
    "    # generate CAM\n",
    "    grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "\n",
    "    cam = np.uint8(255*grayscale_cams[0, :])\n",
    "\n",
    "    cam = cv2.merge([cam, cam, cam])\n",
    "\n",
    "    # display the original image & the associated CAM\n",
    "    images = np.hstack((np.uint8(255*img), cam_image))\n",
    "    PIL.Image.fromarray(images)\n",
    "\n",
    "    return cam_image, predicted_class_acc(img, model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_img(img_gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_color(model, target_layers, targets, img):\n",
    "\n",
    "    cam = GradCAM(model=model, target_layers=target_layers) # use GradCamPlusPlus class\n",
    "\n",
    "    # # Split the image into its RGB channels\n",
    "    b, g, r = cv2.split(img)\n",
    "\n",
    "    # Decrease intensity of the red channel\n",
    "    r = cv2.add(r, 0.4)  # Add 1 to the red channel values\n",
    "\n",
    "    # Clip pixel values to ensure they stay within the range [0, 255]\n",
    "    r = np.clip(r, 0, 1)\n",
    "\n",
    "    # Merge the channels back together\n",
    "    img_color = cv2.merge((b, g, r))\n",
    "\n",
    "    input_tensor = preprocess_image(img_color)\n",
    "\n",
    "    # generate CAM\n",
    "    grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "    cam_image = show_cam_on_image(img_color, grayscale_cams[0, :], use_rgb=True)\n",
    "    cam = np.uint8(255*grayscale_cams[0, :])\n",
    "\n",
    "    cam = cv2.merge([cam, cam, cam])\n",
    "\n",
    "    # display the original image & the associated CAM\n",
    "    images = np.hstack((np.uint8(255*img), np.uint8(255*img_color), cam_image))\n",
    "    PIL.Image.fromarray(images)\n",
    "\n",
    "    return cam_image, predicted_class_acc(img, model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_img(img_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Normal model:\n",
    "\n",
    "# use the pretrained ResNet50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schizophrenia model:\n",
    "\n",
    "model_s = models.resnet50(pretrained=True)\n",
    "model_s.eval()\n",
    "\n",
    "# Modify the fully connected layer for your specific task\n",
    "num_classes = 4  # Modify this based on your classification task\n",
    "model_s.fc = torch.nn.Linear(2048, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "## targets: \n",
    "\n",
    "# fix the target layer (after which we'd like to generate the CAM)\n",
    "target_layers = [model.layer4]\n",
    "target_layers_s = [model_s.layer4]\n",
    "\n",
    "# targets = [ClassifierOutputTarget(1)]\n",
    "\n",
    "# targets = [ClassifierOutputTarget(208), ClassifierOutputTarget(155), ClassifierOutputTarget(254), ClassifierOutputTarget(235)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Predicted class: 208\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 208 is out of bounds for dimension 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[368], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# cv2.imwrite(f'/Users/liviamurray/552_ALproj/output_images/1_n.jpg', output_image)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m predict_arr\u001b[38;5;241m.\u001b[39mappend([output_label, label_imagenet[i]])\n\u001b[0;32m---> 24\u001b[0m output_image, output_label \u001b[38;5;241m=\u001b[39m \u001b[43mschizo_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layers_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/liviamurray/552_ALproj/output_images/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_imagenet[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_label[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_s.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, output_image)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# cv2.imwrite(f'/Users/liviamurray/552_ALproj/output_images/1_s.jpg', output_image)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[360], line 7\u001b[0m, in \u001b[0;36mschizo_model\u001b[0;34m(model, target_layers, targets, img)\u001b[0m\n\u001b[1;32m      4\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m preprocess_image(img)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# generate CAM\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m grayscale_cams \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m cam_image \u001b[38;5;241m=\u001b[39m show_cam_on_image(img, grayscale_cams[\u001b[38;5;241m0\u001b[39m, :], use_rgb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m cam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8(\u001b[38;5;241m255\u001b[39m\u001b[38;5;241m*\u001b[39mgrayscale_cams[\u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/552env/lib/python3.11/site-packages/pytorch_grad_cam/base_cam.py:192\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(\n\u001b[1;32m    190\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/552env/lib/python3.11/site-packages/pytorch_grad_cam/base_cam.py:92\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muses_gradients:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 92\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     94\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# In most of the saliency attribution papers, the saliency is\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# computed with a single target layer.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Commonly it is the last convolutional layer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# use all conv layers for example, all Batchnorm layers,\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# or something else.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/552env/lib/python3.11/site-packages/pytorch_grad_cam/base_cam.py:92\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muses_gradients:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 92\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m                \u001b[38;5;28;01mfor\u001b[39;00m target, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(targets, outputs)])\n\u001b[1;32m     94\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# In most of the saliency attribution papers, the saliency is\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# computed with a single target layer.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Commonly it is the last convolutional layer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# use all conv layers for example, all Batchnorm layers,\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# or something else.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/552env/lib/python3.11/site-packages/pytorch_grad_cam/utils/model_targets.py:12\u001b[0m, in \u001b[0;36mClassifierOutputTarget.__call__\u001b[0;34m(self, model_output)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_output):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_output\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategory\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 208 is out of bounds for dimension 0 with size 4"
     ]
    }
   ],
   "source": [
    "names = ['Lab', 'ST', 'Pug', 'GS']\n",
    "label_imagenet = [208, 208, 208, 155, 155, 155, 254, 254, 254, 235, 235, 235]\n",
    "num_label = [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
    "output_im_arr = np.array([])\n",
    "predict_arr = []\n",
    "\n",
    "for i in range(1): #, len(label_imagenet)):\n",
    "    targets = [ClassifierOutputTarget(label_imagenet[i])] ## THIS IS THE ISSUE\n",
    "    # targets = [ClassifierOutputTarget(1)]\n",
    "\n",
    "    # print(f'Opening...', f'/Users/liviamurray/552_ALproj/dogs/{names[(i) // 3]}/{num_label[i]}.jpeg')\n",
    "    img = np.array(PIL.Image.open(f'/Users/liviamurray/552_ALproj/dogs/{names[(i) // 3]}/{num_label[i]}.jpeg'))\n",
    "    # img = np.array(PIL.Image.open(f'/Users/liviamurray/552_ALproj/goldfish-photo.jpg'))\n",
    "\n",
    "    img = cv2.resize(img, (300,300))\n",
    "    img = np.float32(img) / 255\n",
    "    \n",
    "    output_image, output_label = normal_model(model, target_layers, targets, img)\n",
    "    cv2.imwrite(f'/Users/liviamurray/552_ALproj/output_images/{label_imagenet[i]}_{num_label[i]}_n2.jpg', output_image)\n",
    "    # cv2.imwrite(f'/Users/liviamurray/552_ALproj/output_images/1_n.jpg', output_image)\n",
    "\n",
    "    predict_arr.append([output_label, label_imagenet[i]])\n",
    "        \n",
    "    output_image, output_label = schizo_model(model_s, target_layers_s, targets, img)\n",
    "    cv2.imwrite(f'/Users/liviamurray/552_ALproj/output_images/{label_imagenet[i]}_{num_label[i]}_s.jpg', output_image)\n",
    "    # cv2.imwrite(f'/Users/liviamurray/552_ALproj/output_images/1_s.jpg', output_image)\n",
    "\n",
    "    predict_arr.append([output_label, label_imagenet[i]])\n",
    "        \n",
    "    output_image, output_label = gauss_blur(model_s, target_layers_s, targets, img)\n",
    "    cv2.imwrite(f'/Users/liviamurray/552_ALproj/output_images/{label_imagenet[i]}_{num_label[i]}_g.jpg', output_image)\n",
    "    # cv2.imwrite(f'/Users/liviamurray/552_ALproj/output_images/1_g.jpg', output_image)\n",
    "\n",
    "    predict_arr.append([output_label, label_imagenet[i]])\n",
    "        \n",
    "    output_image, output_label = aug_color(model_s, target_layers_s, targets, img)\n",
    "    cv2.imwrite(f'/Users/liviamurray/552_ALproj/output_images/{label_imagenet[i]}_{num_label[i]}_c.jpg', output_image)\n",
    "    # cv2.imwrite(f'/Users/liviamurray/552_ALproj/output_images/1_c.jpg', output_image)\n",
    "    predict_arr.append([output_label, label_imagenet[i]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly predicted number: 1 / 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[208, 208], [0, 208], [0, 208], [0, 208]]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_arr\n",
    "corr = 0\n",
    "\n",
    "for i, arr in enumerate(predict_arr):\n",
    "    if arr[0] == arr[1]:\n",
    "        corr += 1\n",
    "\n",
    "print(f\"Correctly predicted number: {corr} / {len(label_imagenet)}\")\n",
    "\n",
    "predict_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "552env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
